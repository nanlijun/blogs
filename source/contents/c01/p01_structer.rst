=====================================
1 爬虫
=====================================

1.1 Python爬虫介绍
-------------------------------------

**1.什么是爬虫？**

 说白了就是爬虫可以模拟浏览器的行为做你想做的事，订制化自己搜索和下载的内容，并实现自动化的操作。比如浏览器可以下载小说，但是有时候并不能批量
 下载，那么爬虫的功能就有用武之地了。

**2.爬虫学习路线**

 | 首先学会基本的Python语法知识;
 | 学习Python爬虫常用到的几个重要内置库urllib, http等，用于下载网页;
 | 学习正则表达式re、BeautifulSoup（bs4）、Xpath（lxml）等网页解析工具;
 | 开始一些简单的网站爬取（博主从百度开始的，哈哈），了解爬取数据过程;
 | 了解爬虫的一些反爬机制，header，robot，时间间隔，代理ip，隐含字段等;
 | 学习一些特殊网站的爬取，解决登录、Cookie、动态网页等问题;
 | 了解爬虫与数据库的结合，如何将爬取数据进行储存;
 | 学习应用Python的多线程、多进程进行爬取，提高爬虫效率; 
 | 学习爬虫的框架，Scrapy、PySpider等;
 | 学习分布式爬虫（数据量庞大的需求）.

**3.爬虫过程**

 |   其实，爬虫的过程和浏览器浏览网页的过程是一样的。道理大家应该都明白，就是当我们在键盘上输入网址点击搜索之后，通过网络首先会经过DNS服务器，分析网址的域名，找到了真正的服务器。然后我们通过HTTP协议对服务器发出GET或POST请求，若请求成功，我们就得到了我们想看到的网页，一般都是用HTML, CSS, JS等前端技术来构建的，若请求不成功，服务器会返回给我们请求失败的状态码，常见到的503，403等。

 |   爬虫的过程亦是如此，通过对服务器发出请求得到HTML网页，然后对下载的网页进行解析，得到我们想要的内容。当然，这是一个爬虫过程的一个概况，其中还有很多细节的东西需要我们处理的，这些在后续会继续与大家分享。

 |   了解了爬虫的基本过程后，就可以开始我们真正的爬虫之旅了。
 |   以上内容摘自https://segmentfault.com/a/1190000012681700

1.2 Python爬虫---urllib库
-------------------------------------

**urllib库**

 | Python有一个内置的urllib库，使用这个内置库可以完成向服务器发出请求并获得网页的功能。
 | Python3.x urllib库的结构相对于Python2.x有一些出入，Python2.x中使用的urllib2和urllib库，而Python3.x中合并成一个唯一的urllib库。

::

 >>>importurllib
 >>>dir(urllib)
 ['__builtins__','__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__','__path__', '__spec__', 'error', 'parse', 'request', 'response']

|   可以看到urllib除了以双下划线开头结尾的内置属性外，还有4个重要的属性，分别是error，parse，request，response。

**1.request的使用**

 | request请求最简单的操作是用urlopen方法，代码如下：

::

 >>>import urllib.request
 >>>response = urllib.request.urlopen('http://python.org/')
 >>>result = response.read().decode('utf-8')
 >>>print(result)

 | 得到的就是我们想要的html的网页了。
